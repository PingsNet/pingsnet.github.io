name: Auto Generate Sitemap & Robots.txt
on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * *' # روزانه در نیمه‌شب اجرا می‌شود

permissions:
  contents: write # دسترسی نوشتن برای کامیت خودکار

jobs:
  generate-seo-files:
    runs-on: ubuntu-latest
    steps:
      # مرحله 1: دریافت کدهای ریپو
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # دریافت تمام تاریخچه کامیت‌ها

      # مرحله 2: تولید خودکار sitemap.xml
      - name: Generate Sitemap
        uses: cicirello/generate-sitemap@v1
        with:
          base-url-path: "https://pingsnet.github.io" # آدرس سایت شما
          change-frequency: "weekly" # فرکانس تغییرات
          priority: "0.8" # اولویت پیش‌فرض
          include-extension: ".html" # فقط فایل‌های HTML
          exclude-extension: ".md" # حذف فایل‌های Markdown
          trailing-slash: false # بدون اسلش در انتها

      # مرحله 3: ایجاد فایل robots.txt
      - name: Create robots.txt
        run: |
          cat << EOF > robots.txt
          User-agent: *
          Allow: /
          Disallow: /private/
          Disallow: /temp/
          Disallow: /drafts/
          Crawl-delay: 10
          
          Sitemap: https://pingsnet.github.io/sitemap.xml
          EOF

      # مرحله 4: ارسال تغییرات به ریپو
      - name: Commit and Push Changes
        run: |
          # تنظیمات کاربر Git
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          
          # اضافه کردن فایل‌ها
          git add sitemap.xml robots.txt
          
          # کامیت تغییرات
          git diff --quiet && git diff --staged --quiet || git commit -m "Auto-generated SEO files [skip ci]"
          
          # ارسال تغییرات
          git push origin main
